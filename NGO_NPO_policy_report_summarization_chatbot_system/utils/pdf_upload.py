import streamlit as st
import requests
import os
import json
import io
import tiktoken
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader, PdfWriter
from langchain_upstage import ChatUpstage
load_dotenv()

UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
MAX_FILE_SIZE = 20 * 1024 * 1024  # 20MB in bytes
MAX_PAGES_PER_CHUNK = 90  # Upstage Synchronous API ì œí•œ: 100í˜ì´ì§€ (ì•ˆì •ì„±ì„ ìœ„í•´ 90í˜ì´ì§€ë¡œ ì„¤ì •)
MAX_TOKENS = 30000  # í† í° ì œí•œ

upstage_llm = ChatUpstage(
    api_key=UPSTAGE_API_KEY,
    model="solar-pro2-preview"
)

def count_tokens(text: str) -> int:
    """Upstage ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    return upstage_llm.get_num_tokens(text)

def truncate_text_by_tokens(text: str, max_tokens: int) -> tuple[str, int, int]:
    """Upstage í† í° ì¹´ìš´íŒ…ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìë¦…ë‹ˆë‹¤."""
    original_token_count = count_tokens(text)
    
    if original_token_count <= max_tokens:
        return text, original_token_count, original_token_count
    
    # ì´ì§„ íƒìƒ‰ìœ¼ë¡œ ì ì ˆí•œ ê¸¸ì´ ì°¾ê¸°
    left, right = 0, len(text)
    best_text = text[:right//2]
    
    while left < right:
        mid = (left + right + 1) // 2
        candidate_text = text[:mid]
        token_count = count_tokens(candidate_text)
        
        if token_count <= max_tokens:
            best_text = candidate_text
            left = mid
        else:
            right = mid - 1
    
    # ë¬¸ì¥ì´ ì¤‘ê°„ì— ì˜ë¦¬ëŠ” ê²ƒì„ ë°©ì§€
    sentences = best_text.split('.')
    if len(sentences) > 1:
        best_text = '.'.join(sentences[:-1]) + '.'
    
    final_token_count = count_tokens(best_text)
    return best_text, original_token_count, final_token_count

def split_pdf_by_pages(file_bytes, max_size_bytes):
    """PDFë¥¼ í˜ì´ì§€ë³„ë¡œ ë¶„í• í•˜ì—¬ ê° ë¶€ë¶„ì´ ìµœëŒ€ í¬ê¸°ì™€ í˜ì´ì§€ ìˆ˜ë¥¼ ë„˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤."""
    try:
        pdf_reader = PdfReader(io.BytesIO(file_bytes))
        total_pages = len(pdf_reader.pages)
        
        if total_pages == 0:
            return None, "PDFì— í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸ ë° ê²½ê³ 
        if total_pages > MAX_PAGES_PER_CHUNK:
            st.warning(f"âš ï¸ ì´ {total_pages}í˜ì´ì§€ì…ë‹ˆë‹¤. {MAX_PAGES_PER_CHUNK}í˜ì´ì§€ì”© ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        # í‰ê·  í˜ì´ì§€ í¬ê¸° ì¶”ì •
        avg_page_size = len(file_bytes) / total_pages
        pages_per_chunk_by_size = max(1, int(max_size_bytes / avg_page_size))
        
        # í˜ì´ì§€ ì œí•œê³¼ í¬ê¸° ì œí•œ ì¤‘ ë” ì‘ì€ ê°’ ì‚¬ìš©
        pages_per_chunk = min(pages_per_chunk_by_size, MAX_PAGES_PER_CHUNK)
        
        chunks = []
        current_chunk_start = 0
        
        while current_chunk_start < total_pages:
            # ì²­í¬ ìƒì„±
            pdf_writer = PdfWriter()
            current_chunk_end = min(current_chunk_start + pages_per_chunk, total_pages)
            
            # í˜ì´ì§€ ì¶”ê°€
            for page_num in range(current_chunk_start, current_chunk_end):
                pdf_writer.add_page(pdf_reader.pages[page_num])
            
            # ë©”ëª¨ë¦¬ì— PDF ì‘ì„±
            output_buffer = io.BytesIO()
            pdf_writer.write(output_buffer)
            chunk_bytes = output_buffer.getvalue()
            output_buffer.close()
            
            # ì²­í¬ í¬ê¸°ê°€ ì—¬ì „íˆ ë„ˆë¬´ í° ê²½ìš° í˜ì´ì§€ë¥¼ ë” ì¤„ì„
            if len(chunk_bytes) > max_size_bytes and pages_per_chunk > 1:
                pages_per_chunk = max(1, min(pages_per_chunk // 2, MAX_PAGES_PER_CHUNK))
                continue
            
            # í˜ì´ì§€ ìˆ˜ ì œí•œ ì²´í¬
            actual_pages = current_chunk_end - current_chunk_start
            if actual_pages > MAX_PAGES_PER_CHUNK:
                return None, f"ì²­í¬ í¬ê¸°ê°€ í˜ì´ì§€ ì œí•œ({MAX_PAGES_PER_CHUNK}í˜ì´ì§€)ì„ ì´ˆê³¼í•©ë‹ˆë‹¤."
            
            chunks.append({
                'data': chunk_bytes,
                'pages': f"{current_chunk_start + 1}-{current_chunk_end}",
                'page_count': actual_pages,
                'size': len(chunk_bytes)
            })
            
            current_chunk_start = current_chunk_end
        
        return chunks, None
        
    except Exception as e:
        return None, f"PDF ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def process_single_document(file_bytes, force_ocr: bool, chunk_info=None):
    """ë‹¨ì¼ ë¬¸ì„œ(ë˜ëŠ” ë¬¸ì„œ ì²­í¬)ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    url = "https://api.upstage.ai/v1/document-digitization"
    headers = {"Authorization": f"Bearer {UPSTAGE_API_KEY}"}
    files = {
        "document": ("document.pdf", file_bytes, "application/pdf")
    }
    data = {
        "ocr": "force" if force_ocr else "auto",
        "coordinates": "true",
        "chart_recognition": "false",
        "output_formats": json.dumps(["html"]),
        "model": "document-parse",
        "base64_encoding": json.dumps([])
    }

    try:
        response = requests.post(url, headers=headers, files=files, data=data)
        response.raise_for_status()
        result = response.json()

        html_content = result.get("content", {}).get("html", "").strip()

        if not html_content:
            return None, f"ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {chunk_info or ''}"
        
        soup = BeautifulSoup(html_content, "html.parser")
        plain_text = soup.get_text("\n")

        # í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ ì ìš©
        original_token_count = count_tokens(plain_text)
        
        if original_token_count > MAX_TOKENS:
            truncated_text, original_tokens, final_tokens = truncate_text_by_tokens(plain_text, MAX_TOKENS)
            st.warning(f"âš ï¸ í…ìŠ¤íŠ¸ê°€ í† í° ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì˜ë¦¼: {original_tokens:,} â†’ {final_tokens:,} í† í° {chunk_info or ''}")
            return truncated_text, None
        else:
            st.info(f"ğŸ“Š ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {original_token_count:,} í† í° {chunk_info or ''}")
            return plain_text, None
    
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 413:
            return None, f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ (413 ì˜¤ë¥˜). {chunk_info or ''}"
        else:
            return None, f"HTTP ì˜¤ë¥˜ {e.response.status_code}: {str(e)} {chunk_info or ''}"
    except Exception as e:
        return None, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)} {chunk_info or ''}"

def process_document(file_bytes, force_ocr: bool):
    """
    ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. íŒŒì¼ í¬ê¸°ê°€ 20MBë¥¼ ì´ˆê³¼í•˜ê±°ë‚˜ í˜ì´ì§€ê°€ 90í˜ì´ì§€ë¥¼ ì´ˆê³¼í•˜ë©´ ìë™ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    """
    file_size_mb = len(file_bytes) / (1024 * 1024)
    
    # ë¨¼ì € í˜ì´ì§€ ìˆ˜ í™•ì¸ (ë¶„í•  í•„ìš”ì„± íŒë‹¨ì„ ìœ„í•´)
    try:
        pdf_reader = PdfReader(io.BytesIO(file_bytes))
        total_pages = len(pdf_reader.pages)
    except Exception as e:
        return None, f"PDF í˜ì´ì§€ ì •ë³´ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"
    
    # íŒŒì¼ í¬ê¸°ì™€ í˜ì´ì§€ ìˆ˜ ì²´í¬
    needs_splitting = len(file_bytes) > MAX_FILE_SIZE or total_pages > MAX_PAGES_PER_CHUNK
    
    if not needs_splitting:
        # 20MB ì´í•˜ì´ê³  90í˜ì´ì§€ ì´í•˜ì¸ ê²½ìš° ì§ì ‘ ì²˜ë¦¬
        st.info(f"ğŸ“„ íŒŒì¼ ì •ë³´: {file_size_mb:.2f}MB, {total_pages}í˜ì´ì§€ - ì§ì ‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        return process_single_document(file_bytes, force_ocr)
    
    # 20MB ì´ˆê³¼ì´ê±°ë‚˜ 90í˜ì´ì§€ ì´ˆê³¼ì¸ ê²½ìš° ë¶„í•  ì²˜ë¦¬
    split_reasons = []
    if len(file_bytes) > MAX_FILE_SIZE:
        split_reasons.append(f"íŒŒì¼ í¬ê¸° {file_size_mb:.2f}MB > 20MB")
    if total_pages > MAX_PAGES_PER_CHUNK:
        split_reasons.append(f"í˜ì´ì§€ ìˆ˜ {total_pages}í˜ì´ì§€ > {MAX_PAGES_PER_CHUNK}í˜ì´ì§€")
    
    st.warning(f"ğŸ“„ ë¶„í•  ì²˜ë¦¬ ì‚¬ìœ : {', '.join(split_reasons)}")
    
    # PDF ë¶„í• 
    with st.spinner("PDFë¥¼ ë¶„í• í•˜ëŠ” ì¤‘..."):
        chunks, error = split_pdf_by_pages(file_bytes, MAX_FILE_SIZE)
        
        if error:
            return None, error
        
        if not chunks:
            return None, "PDF ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    st.info(f"ğŸ“‘ PDFë¥¼ {len(chunks)}ê°œ ë¶€ë¶„ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    
    # ê° ì²­í¬ë¥¼ ì²˜ë¦¬
    all_text_parts = []
    total_tokens = 0
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, chunk in enumerate(chunks):
        progress = (i + 1) / len(chunks)
        status_text.text(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {chunk['pages']} í˜ì´ì§€ ({chunk['page_count']}í˜ì´ì§€, {chunk['size'] / (1024*1024):.2f}MB)")
        progress_bar.progress(progress)
        
        chunk_info = f"(í˜ì´ì§€ {chunk['pages']}, {chunk['page_count']}í˜ì´ì§€)"
        text_part, error = process_single_document(chunk['data'], force_ocr, chunk_info)
        
        if error:
            st.error(f"ì²­í¬ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {error}")
            continue
        
        if text_part:
            # ê° ì²­í¬ì˜ í† í° ìˆ˜ ëˆ„ì  ê³„ì‚°
            chunk_tokens = count_tokens(text_part)
            total_tokens += chunk_tokens
            
            all_text_parts.append(f"\n\n=== í˜ì´ì§€ {chunk['pages']} ({chunk['page_count']}í˜ì´ì§€, {chunk_tokens:,} í† í°) ===\n\n{text_part}")
    
    # ì§„í–‰ë¥  ë°” ì •ë¦¬
    progress_bar.empty()
    status_text.empty()
    
    if not all_text_parts:
        return None, "ëª¨ë“  PDF ì²­í¬ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    # ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    combined_text = "\n".join(all_text_parts)
    
    # ì „ì²´ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ìµœì¢… í™•ì¸ ë° ì œí•œ ì ìš©
    final_token_count = count_tokens(combined_text)
    
    if final_token_count > MAX_TOKENS:
        st.warning(f"âš ï¸ ì „ì²´ í…ìŠ¤íŠ¸ê°€ í† í° ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì˜ë¦¼: {final_token_count:,} â†’ {MAX_TOKENS:,} í† í°")
        truncated_text, original_tokens, final_tokens = truncate_text_by_tokens(combined_text, MAX_TOKENS)
        combined_text = truncated_text
        final_token_count = final_tokens
    
    processed_pages = sum(chunk['page_count'] for chunk in chunks if any(f"í˜ì´ì§€ {chunk['pages']}" in part for part in all_text_parts))
    st.success(f"âœ… {len(chunks)}ê°œ ë¶€ë¶„ ì¤‘ {len(all_text_parts)}ê°œ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ì™„ë£Œ! (ì´ {processed_pages}í˜ì´ì§€, {final_token_count:,} í† í°)")
    
    return combined_text, None
    
